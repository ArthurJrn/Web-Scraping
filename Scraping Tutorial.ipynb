{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyst Scraper: a quick tutorial on web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a quick tutorial on how to scrap a website. I am not a web scraping expert at all, I am mainly still a beginner, so this tutorail is aimed at beginners mostly. However, I tried not to focus only on the __code__ but also on other aspects of web scraping. You can find plenty of complete tutorials for the use of Beautiful Soup or Selenium online. Instead, I wanted to show a glimpse of how to scrap a website, some useful tricks I learnt during my work. I am __machine learning researcher__, so I am not at all a scraping specialist nor a web programer, but I think web scraping is a useful trick to learn for data scientists and machine learning students to create databases or simply gather data. For other examples, you can check my other scrapers on my GitHub, they are more detailed from a syntaxic point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web scraping ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to start coding, it is important to understand what is a web scraper, how and __when__ to use it. More important, web scraping has a tricky __legal__ aspect, so be sure to read these few lines before you start playing with data.\n",
    "\n",
    "## What is a web scraper ?\n",
    "\n",
    "A web scraper is a program that __extracts__ data from the World Wide Web. In all my examples, I use web scraper to download images to create databases, but I can be used to extract pretty much any type of data you can find online.\n",
    "Web scraping is different from __web crawling__, which is basically gathering and indexing every byte of info you can find on a website. As a crawler visits all the links on a page, I still use the term __crawl__ to describe that a scraper is visiting a link.\n",
    "\n",
    "## Why would you scrap a website ?\n",
    "\n",
    "There are many reasons to scrap a website. Personnally, I started to scrap to create databases for my __machine learning__ projects. But as I got into web scraping, I enjoyed it so much that I started to scrap some websites for fun (like Google Trends or op.gg for instance).\n",
    "\n",
    "## The big question: is web scraping legal ?\n",
    "\n",
    "Wellll... This is a good one. I am no lawyer, so I can't give you a detailed answer here, but here's what I understood after spending some time looking for a definitive answer. There is no __definitive__ answer. It heavily depends on what you do with the data you scrap. Here's a good blog about the subject : https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/. But always keep in mind that the fact that the data is publicly accesible __does not mean you have the right to sracp it and use it for your own projects__. Simples advices are:\n",
    "-  If you're not sure, ask the owner of the website for his __permisssion__ to scrap it\n",
    "-  If you think the data might be sensible, __ask a specialized lawyer__\n",
    "-  Follow the rules specified in the __robots.txt__ file. Every website has a basic HTML page describing its policy about web scraping. You can find it at _http://www.website.com/robots.txt_. You can have nice surprises, as some websites allow web scraping to an extend (ex: op.gg for League of Legends statistics).\n",
    "-  Be __polite__. I will insist a lot on this during this tutorial, but servers are not made to handle 10 requests a second per user. This is not a normal behaviour for a human being, and it can seriously harm a website. If you're taking their data - especially without permission - at least do it right.\n",
    "   \n",
    "\n",
    "# Part 2: \"Naive\" web scraping\n",
    "\n",
    "This \"Naive\" part is very important. I call it naive because it requires some manual work to understand how to retrieve the data from a website, but this step is essential to understand how a scraper works, especially if - like me - you don't have much knowledge on webprograming.\n",
    "\n",
    "Keep in mind that this notebook is not designed to be a complete all-in-one tutorial for beginners. You can find plenty of tutorials for Beautiful Soup or the Selenium syntax on the web, so I won't detail each ligne of code I wrote. This notebook is written by a beginner for beginners, it is here to give a glimpse on how to scrap a website, and some useful tips I found by myself.\n",
    "\n",
    "During this tutorial, we will try to get images from _lyst.fr_, an online fashion shopping website.\n",
    "\n",
    "First, let's import the useful librairies we're gonna use during our project. They are all straight-forward to install using __pip__, except Selenium which needs you to install a bit more but you can find everything on the documentation of Selenium online.\n",
    "During this tutorial, I use Beautiful Soup, a Python librairy for extracting data from HTML files. Here's the link to the documentation https://www.crummy.com/software/BeautifulSoup/bs4/doc/.\n",
    "Beautiful Soup is a very useful and complete tool for basic webscraping, however some more complex tasks cannot be achieved. For example, _lyst.fr_ features an __infinite scroll__ which requires us to use an automated browser using Selenium (Documentation at https://www.seleniumhq.org/ ). Tip for installing: go to https://github.com/mozilla/geckodriver/releases and get the geckodriver corresponding to your Mozilla Browser if you use Mozilla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import unidecode\n",
    "import shutil\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you scrap a website using Selenium or Beautiful, __your__ computer is accesing the data. It seems dumb to state this, but remember that a website expects human user to access its data, not automated bots. That's why we need to disguise our bot to make him look like a normal human user (_spoiler: we won't succeed_).\n",
    "\n",
    "Headers are the __fingerprint__ of your browser. I always specify my true headers (in my case, Firefox Headers) when I scrap a website, because it doesn't give much info on myself, and it seems more realistic. You can find your headers using this website https://www.whatismybrowser.com/detect/what-http-headers-is-my-browser-sending, look for the __USER_AGENT__ info.\n",
    "\n",
    "We're gonna try with one headers first. If this doesn't work, we can try using proxy rotation and multi-headers to hide our identity. But given the fact that we are not going to scrap a lot of sensitive data, we might be able to go anonymous. I've been nlocked from several websites beacause I was careless before so we should be careful. Anyway, when you crawl a website, remember that your request are not normal behaviour from a human user. You may do 10 request/second, which is way too much for a human user and might harm the website server. Please, be polite and use some sleep to relieve the servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your web browser's headers\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0'}\n",
    "\n",
    "# Specify the location to save the data\n",
    "DownloadPath = \"/media/arthur/DATA/Databases/LystScrapingH\"\n",
    "\n",
    "# We create a dictionary with all cat. and subcat. of the Lyst catalogue\n",
    "words_to_search =  {\n",
    "    'shirts':['casual+shirts','formal+shirts'],\n",
    "    'suits':['2+piece+suits', '3+piece+suits','evening+suits'],\n",
    "    'jeans':['bootcut+jeans', 'straight+jeans', 'relaxed+jeans', 'tapered+jeans', 'skinny+jeans', 'slim+jeans'],\n",
    "    'coats':['trench+coats', 'short+coats', 'long+coats',  'parka+coats'],\n",
    "    'pants':['casual+pants', 'formal+pants'],\n",
    "    'beachwear':['trunks', 'boardshorts'],\n",
    "    'knitwear':['cardigans','v+neck', 'crew+neck', 'turtlenecks', 'zipped+sweaters', 'sleeveless+sweaters'],\n",
    "    'shorts':['bermudas+shorts', 'formal+shorts', 'casual+shorts','cargo+shorts'],\n",
    "    'underwear':['boxers','socks', 'undershirt','briefs'],\n",
    "    'sweats':['sweatpants', 'sweatshorts','tracksuits','sweatshirts', 'hoodies'],\n",
    "    't-shirts':['polo+shirts','sleeveless+t-shirts','short+sleeve+t-shirts','long+sleeve+t-shirts'],\n",
    "    'jackets':['formal+jackets', 'leather+jackets','waistcoast','casual+jackets','parka+jackets'],\n",
    "    'nightwear':['pyjamas', 'robes']\n",
    "}\n",
    "\n",
    "# Create the tree in your hard disk \n",
    "for category in words_to_search:\n",
    "    os.makedirs(DownloadPath + \"/\" + category)\n",
    "    subcategories = words_to_search[category]\n",
    "    for clothing in subcategories:\n",
    "        os.makedirs(DownloadPath + \"/\" + category + \"/\" + clothing)\n",
    "print(\"Tree created\")\n",
    "\n",
    "url_download_base = \"https://cdna.lystit.com/520/650/n/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the __url_download_base__ variable. This is common when scraping images: websites only show you small images, so when you __inspect__ the lement, you get the source of a small image. However, every website has the image saved somewhere in good quality, you just need to dig a little to find it.\n",
    "\n",
    "First, we're gonna do a little test on just the 'Shirts' category. Our goal is to download all the images from this category and to save them locally on our computer. Selenium is gonna create an instance of our web browser (in my case, Mozilla Firefox) which will be controlled by this script. Be careful which retriever you use for the image, for example I first tried urllib.urlretrieve, but I got blocked by the website. If that happens, try another retriever (ex here, requests) or build your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scrap = words_to_search['shirts']\n",
    "    \n",
    "# Use selenium to create a firefox instance\n",
    "driver = webdriver.Firefox()\n",
    "extensions = {\"jpg\", \"jpeg\", \"png\"}\n",
    "\n",
    "# Loop on all the clothes\n",
    "for clothing in to_scrap:\n",
    "    url = \"https://www.lyst.fr/parcourir/vetements-pour-homme/?category=\" + 'shirts' +  \"&subcategory=\" + clothing\n",
    "    driver.get(url)\n",
    "    for _ in range(100):\n",
    "        driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "        \n",
    "        # If the website is a bit buggy, be smart\n",
    "        time.sleep(0.7)\n",
    "        driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "        \n",
    "        # Don't forget this sleep to let the image appear on the page\n",
    "        time.sleep(1.2)\n",
    "        \n",
    "    # Now we download all the images. BE POLITE AND DON'T GET BAN.\n",
    "    images = driver.find_elements_by_xpath('//div[contains(@class, \"product-card__image\")]')\n",
    "    i = 0\n",
    "    for img in images:\n",
    "        time.sleep(0.1)\n",
    "        i+=1\n",
    "        # Find the image and its src\n",
    "        image = img.find_element_by_tag_name(\"img\")\n",
    "        print(\"Dowloading image no\", i, \"/\", len(images))\n",
    "        \n",
    "        # We modify the src to access the full size image\n",
    "        try:\n",
    "            image_url = url_download_base + image.get_attribute(\"src\")[35::]\n",
    "        except:\n",
    "            print(\"Source not found.\")\n",
    "            continue\n",
    "        image_name = image_url.split(\"/\")[-1]\n",
    "        file_name =DownloadPath + '/shirts/' + clothing + \"/\" + image_name\n",
    "        try:\n",
    "            r = requests.get(image_url, stream=True, headers=headers)\n",
    "            f = open(file_name, 'wb')\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "        except:\n",
    "            print(\"Error while downloading the image.\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "    print(\"Downloading over.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a \"basic\" scraper: it only downloaded the images, saving them where we wanted (plus a lot of images can't be downloaded because the source is not specified). Now, we need all the info about the product: retailer, brand, description, save the url... Let's do a smarter scraper, not losing all this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try to get all the data from a single url and save it to a nice dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.lyst.fr/vetements/etoile-isabel-marant-pre-owned-chemise-en-lin/'\n",
    "html = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(html.text, 'lxml')\n",
    "dico = {}\n",
    "\n",
    "# Name of the product\n",
    "name_brand = soup.find('div', {'itemprop':'brand'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "dico['brand'] = unidecode.unidecode(name_brand)\n",
    "print(name_brand)\n",
    "\n",
    "# Short description\n",
    "short_description = soup.find('div', {'itemprop':'name'}).contents[0].string.replace(\"\\n\", \"\")\n",
    "dico['short-description'] = unidecode.unidecode(short_description)\n",
    "print(short_description)\n",
    "\n",
    "# Retailer \n",
    "retailer = soup.find('a', {'reason':'retailer-link'}).contents[1].string[19::].replace(\"\\n\", \"\")\n",
    "dico['retailer'] = unidecode.unidecode(retailer)\n",
    "print(retailer)\n",
    "\n",
    "# Long description\n",
    "long_description = soup.find('div', {'class':'product-description__details text-paragraph mb0'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "dico['long-description'] = unidecode.unidecode(long_description)\n",
    "print(long_description)\n",
    "\n",
    "# Url of the main image\n",
    "url_main_img = soup.find('img', {'class':'image-gallery-main-img'})['src'].replace(\"\\n\", \"\")\n",
    "dico['url-main-image'] = url_main_img\n",
    "print(url_main_img)\n",
    "\n",
    "# Url of thumbnail images (if they exist)\n",
    "other_imgs = soup.find('div', {'is':'gallery-thumbnails'}).contents\n",
    "\n",
    "# Other_imgs is a list of length: 2*NB_OF_OTHER_IMGS+1\n",
    "nb_imgs = int(len(other_imgs)/2)\n",
    "dico['other-images-url'] = []\n",
    "for i in range(1, nb_imgs):\n",
    "    print(other_imgs[2*i+1]['href'])\n",
    "    dico['other-images-url'].append(other_imgs[2*i+1]['href'])\n",
    "    \n",
    "print(dico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Beautiful Soup instance that gets all of this data, we can test it on a full category. We just have to be careful not to download twice the same image. Assuming the fact that an image is located at only one adress, we can take care of this issue by keeping a list of the product we download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same start as before, we need to reach the bottom \n",
    "to_scrap = words_to_search['shirts']\n",
    "Base = 'https://www.lyst.fr'\n",
    "    \n",
    "# Use selenium to create a firefox instance\n",
    "driver = webdriver.Firefox()\n",
    "extensions = {\"jpg\", \"jpeg\", \"png\"}\n",
    "\n",
    "# Create a list to save the data in json format\n",
    "dico = []\n",
    "\n",
    "\n",
    "# Loop on all the clothes\n",
    "for clothing in to_scrap:\n",
    "    print(\"Scrolling to the bottom of the page for category\", clothing, \"...\")\n",
    "\n",
    "    url = \"https://www.lyst.fr/parcourir/vetements-pour-homme/?category=\" + 'shirts' +  \"&subcategory=\" + clothing\n",
    "    driver.get(url)\n",
    "    \n",
    "    for _ in range(number_of_scrolls):\n",
    "        driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "        \n",
    "        # If the website is a bit buggy, be smart\n",
    "        time.sleep(0.7)\n",
    "        driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "        \n",
    "        # Don't forget this sleep to let the image appear on the page\n",
    "        time.sleep(1.2)\n",
    "    print(\"Done !\")\n",
    "    \n",
    "    # Now, we're gonna use the Lyst page of the product\n",
    "    url_products_driver = driver.find_elements_by_xpath('//div[contains(@class, \"product-card__details\")]')\n",
    "    for ind, product in enumerate(list(set(url_products_driver))):\n",
    "        print(\"Products ready to be scraped for category\", clothing, \":\", len(list(set(url_products_driver))))\n",
    "        time.sleep(0.1)\n",
    "        print(\"Extracting data for product no\", ind, \"/\", len(url_products_driver))\n",
    "        dico_product = {}\n",
    "        prod = product.find_element_by_tag_name(\"a\")\n",
    "        # Get the url of the product page\n",
    "        url_prod = prod.get_attribute(\"href\")\n",
    "        \n",
    "        # We use Beautiful Soup now, cause it is simpler to use and doesn't open a browser page\n",
    "        html = requests.get(url_prod, headers = headers)\n",
    "        soup = BeautifulSoup(html.text, 'lxml')\n",
    "        \n",
    "        # Be careful: not all products have every features\n",
    "        if soup.find('div', {'itemprop':'brand'}) is not None:\n",
    "            name_brand = soup.find('div', {'itemprop':'brand'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "            dico_product['brand'] = unidecode.unidecode(name_brand)\n",
    "        if soup.find('div', {'itemprop':'name'}) is not None:\n",
    "            short_description = soup.find('div', {'itemprop':'name'}).contents[0].string.replace(\"\\n\", \"\")\n",
    "            dico_product['short-description'] = unidecode.unidecode(short_description)\n",
    "        if soup.find('a', {'reason':'retailer-link'}) is not None:\n",
    "            retailer = soup.find('a', {'reason':'retailer-link'}).contents[1].string[19::].replace(\"\\n\", \"\")\n",
    "            dico_product['retailer'] = unidecode.unidecode(retailer)\n",
    "        if soup.find('div', {'class':'product-description__details text-paragraph mb0'}) is not None:\n",
    "            long_description = soup.find('div', {'class':'product-description__details text-paragraph mb0'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "            dico_product['long-description'] = unidecode.unidecode(long_description)\n",
    "        # Download the main image with the name of the retailer in its name\n",
    "        url_main_img = soup.find('img', {'class':'image-gallery-main-img'})['src'].replace(\"\\n\", \"\")\n",
    "        \n",
    "        # Choose the name carefully, as several products from different catalogues have the same name...\n",
    "        image_name = + url_main_img.split(\"/\")[-2] + url_main_img.split(\"/\")[-1] \n",
    "        file_name =DownloadPath + '/shirts/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_main_\" + image_name\n",
    "        try:\n",
    "            r = requests.get(url_main_img, stream=True, headers=headers)\n",
    "            f = open(file_name, 'wb')\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "            dico_product['url-main-image'] = url_main_img\n",
    "        except Exception as e:\n",
    "            print(\"Error while downloading the image:\", e)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Download the thumbnail image also with the name of the retailer in their name\n",
    "        other_imgs = soup.find('div', {'is':'gallery-thumbnails'})\n",
    "        dico_product['other-images-url'] = []\n",
    "        \n",
    "        # Not all images have thumbnails images\n",
    "        if other_imgs is not None:\n",
    "            other_imgs = soup.find('div', {'is':'gallery-thumbnails'}).contents\n",
    "            nb_imgs = int(len(other_imgs)/2)\n",
    "            for i in range(1, nb_imgs):\n",
    "                dico_product['other-images-url'].append(other_imgs[2*i+1]['href'])\n",
    "                image_name = other_imgs[2*i+1]['href'].split(\"/\")[-2] + other_imgs[2*i+1]['href'].split(\"/\")[-1]\n",
    "                file_name =DownloadPath + '/shirts/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_\" + str(i) + image_name\n",
    "                if (other_imgs[2*i+1]['href'].replace(\"\\n\", \"\") != url_main_img):\n",
    "                    try:\n",
    "                        r = requests.get(other_imgs[2*i+1]['href'].replace(\"\\n\", \"\"), stream=True, headers=headers)\n",
    "                        f = open(file_name, 'wb')\n",
    "                        shutil.copyfileobj(r.raw, f)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error while downloading the image:\", e)\n",
    "                        continue\n",
    "        \n",
    "        # Put the dico of the product in the list of all data\n",
    "        dico.append(dico_product)\n",
    "        \n",
    "# Save all to  a json file\n",
    "with open(\"shirts.json\", \"w\") as fout:\n",
    "    json.dump(dico, fout)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our \"brutal\" scraper that crawls all the subcategories we chose. This methods works, but has several flaws: \n",
    "-  it is __long__. Selenium's infinite scroll needs the image to load each time we crawl, plus we had to be a little bit tricky by scrolling up to de-bug the website. This is not a major concern if you have time, but it is always good to go fast when processing web scraping. Quick maths: our scraper downloads politely at a speed of ~1 image per second. Given the fact that we plan to download 1 million images, our scraper should run for approximately 11.5 days. This is not ok.\n",
    "-   you can get __blocked__ by the admin of the website's servers. You ask for many requests in a quite short amount of time, so you are likely to get caught.\n",
    "-  to scrap the website this way, a lot of __manual__ work has to be done to understand how the website is built. This takes some time, and can be tricky if the website is obscure or complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete naive webscraper for lyst.fr Men's catalogue\n",
    "\n",
    "# Create a unique list of entries for all the products\n",
    "dico = []\n",
    "\n",
    "# Use selenium to create a firefox instance\n",
    "driver = webdriver.Firefox()\n",
    "extensions = {\"jpg\", \"jpeg\", \"png\"}\n",
    "\n",
    "# Iterate over the keys of the words_to_search dictionary\n",
    "for category in words_to_search:\n",
    "    # Image counter, needed because not every product has the same number of images available\n",
    "    cpt_img = 0\n",
    "    print(\"Processing category\", category, \"...\")\n",
    "    subcategories = words_to_search[category]\n",
    "    # Loop on all the clothes\n",
    "    for clothing in subcategories:\n",
    "        print(\"Scrolling to the bottom of the page for category\", clothing, \"...\")\n",
    "\n",
    "        url = \"https://www.lyst.fr/parcourir/vetements-pour-homme/?category=\" + category +  \"&subcategory=\" + clothing\n",
    "        driver.get(url)\n",
    "        for _ in range(number_of_scrolls):\n",
    "            driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "        \n",
    "            # If the website is a bit buggy, be smart\n",
    "            time.sleep(0.7)\n",
    "            driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "        \n",
    "            # Don't forget this sleep to let the image appear on the page\n",
    "            time.sleep(1.2)\n",
    "        print(\"Done !\")\n",
    "    \n",
    "        # Now, we're gonna use the Lyst page of the product\n",
    "        url_products_driver = driver.find_elements_by_xpath('//div[contains(@class, \"product-card__details\")]')\n",
    "        print(\"Products ready to be scraped for category\", clothing, \":\", len(list(set(url_products_driver))))\n",
    "        for ind, product in enumerate(list(set(url_products_driver))):\n",
    "            time.sleep(0.1)\n",
    "            print(\"Extracting data for product no\", ind, \"/\", len(url_products_driver))\n",
    "            dico_product = {}\n",
    "            prod = product.find_element_by_tag_name(\"a\")\n",
    "            # Get the url of the product page\n",
    "            url_prod = prod.get_attribute(\"href\")\n",
    "        \n",
    "            # We use Beautiful Soup now, cause it is simpler to use and doesn't open a browser page\n",
    "            html = requests.get(url_prod, headers = headers)\n",
    "            soup = BeautifulSoup(html.text, 'lxml')\n",
    "            \n",
    "            # Be careful: not all products have every features\n",
    "            if soup.find('div', {'itemprop':'brand'}) is not None:\n",
    "                if len(soup.find('div', {'itemprop':'brand'}).contents)>1:\n",
    "                    name_brand = soup.find('div', {'itemprop':'brand'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                    dico_product['brand'] = unidecode.unidecode(name_brand)\n",
    "            if soup.find('div', {'itemprop':'name'}) is not None:\n",
    "                short_description = soup.find('div', {'itemprop':'name'}).contents[0].string.replace(\"\\n\", \"\")\n",
    "                dico_product['short-description'] = unidecode.unidecode(short_description)\n",
    "            if soup.find('a', {'reason':'retailer-link'}) is not None:\n",
    "                retailer = soup.find('a', {'reason':'retailer-link'}).contents[1].string[19::].replace(\"\\n\", \"\")\n",
    "                dico_product['retailer'] = unidecode.unidecode(retailer)\n",
    "            if soup.find('div', {'class':'product-description__details text-paragraph mb0'}) is not None:\n",
    "                long_description = soup.find('div', {'class':'product-description__details text-paragraph mb0'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                dico_product['long-description'] = unidecode.unidecode(long_description)\n",
    "            # Download the main image with the name of the retailer in its name\n",
    "            url_main_img = soup.find('img', {'class':'image-gallery-main-img'})['src'].replace(\"\\n\", \"\")\n",
    "            image_name = url_main_img.split(\"/\")[-2] + url_main_img.split(\"/\")[-1]\n",
    "            file_name =DownloadPath + '/'+ category + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_main_\" + image_name\n",
    "            try:\n",
    "                r = requests.get(url_main_img, stream=True, headers=headers)\n",
    "                f = open(file_name, 'wb')\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "                dico_product['url-main-image'] = url_main_img\n",
    "                cpt +=1\n",
    "            except Exception as e:\n",
    "                print(\"Error while downloading the image:\", e)\n",
    "                continue\n",
    "\n",
    "        \n",
    "            # Download the thumbnail image also with the name of the retailer in their name\n",
    "            other_imgs = soup.find('div', {'is':'gallery-thumbnails'})\n",
    "            dico_product['other-images-url'] = []\n",
    "        \n",
    "            # Not all images have thumbnails images\n",
    "            if other_imgs is not None:\n",
    "                other_imgs = soup.find('div', {'is':'gallery-thumbnails'}).contents\n",
    "                nb_imgs = int(len(other_imgs)/2)\n",
    "                for i in range(1, nb_imgs):\n",
    "                    dico_product['other-images-url'].append(other_imgs[2*i+1]['href'])\n",
    "                    image_name = other_imgs[2*i+1]['href'].split(\"/\")[-2] + other_imgs[2*i+1]['href'].split(\"/\")[-1] \n",
    "                    file_name =DownloadPath + '/' + category + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_\" + str(i) + image_name\n",
    "                    if (other_imgs[2*i+1]['href'].replace(\"\\n\", \"\") != url_main_img):\n",
    "                        try:\n",
    "                            r = requests.get(other_imgs[2*i+1]['href'].replace(\"\\n\", \"\"), stream=True, headers=headers)\n",
    "                            f = open(file_name, 'wb')\n",
    "                            shutil.copyfileobj(r.raw, f)\n",
    "                            cpt +=1\n",
    "                        except Exception as e:\n",
    "                            print(\"Error while downloading the image:\", e)\n",
    "                            continue\n",
    "                            \n",
    "    # Get the number of images downloaded for each category\n",
    "    print(\"Downloaded\", cpt, \"images from category\", category)\n",
    "\n",
    "                    \n",
    "            # Put the dico of the product in the list of all data\n",
    "            dico.append(dico_product)\n",
    "        \n",
    "# Save all to  a json file\n",
    "with open(\"lyst.json\", \"w\") as fout:\n",
    "    json.dump(dico, fout)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using the inspection tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 2, we implemented a basic web scraper. It required some sweat to understand how the website is built, inspect many elements... In short, it was enlightening but somewhat long. If we want to get faster to the data, or simply to spend less time inspecting element and infinitely scrolling, we need to be smarter ie to inspect our website more in depth.\n",
    "\n",
    "To inspect a website, I use Chrome - just a personal preference, everything I am going to explain work on Mozilla or IE as well - because its interface seems more friendly to me. Until now, we only used the __elements__ tab of the browser inspect tool. Now, the other tabs can also be very useful. In our case, we're gonna look at the __Network__ tab. This tab records all network requests. This is especiall for infinite scrolling. When you scroll down the page, new images appear (this is trackable with the network tab). This is nothing spectacular, but everytime you reach the bottom of the page, new images need to be downloaded and that's where it gets interesting. In the record of network activity, we find a call to an HTML page that is not the link to an image. It is a page called _\"parcourir\"_, try to open it. You see an HTML page filled with info. When you look closely, this is actually a dictionary containing all the info available for the products loaded. \n",
    "\n",
    "The url is https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2Fparcourir%2Fchemises-pour-homme%2F%3Fpage%3D5. After some quick test, we realize that we can load all the products by modiffying only the last figure. I can go up to 100, let's see how many products we can get using this methods (on the category 'Shirts' to compare with our first scraper).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HTML file and process it. \n",
    "url = 'https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2Fparcourir%2Fchemises-pour-homme%2F%3Fpage%3D50'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'lxml')\n",
    "text_data = soup.text\n",
    "\n",
    "# Try to access some useful data, it is a bit messy at first glance\n",
    "dico = json.loads(text_data)\n",
    "for product in dico['data']['feed_items']:\n",
    "    dico_product = {}\n",
    "    dico_product[\"id\"] = product['product_card']['id']\n",
    "    dico_product[\"price\"] = product['product_card']['full_price_with_currency_symbol']\n",
    "    dico_product[\"retailer\"] = product['product_card']['retailer_name']\n",
    "    dico_product[\"designer\"] = product['product_card']['designer_name']\n",
    "    # We could keep all this data, but some are not useful for us\n",
    "\n",
    "# Let's first check that we have a same number of products using Selenium and this method on the category 'Shirts'\n",
    "url_dico = 'https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2Fparcourir%2Fchemises-pour-homme%2F%3Fsubcategory%3Dcasual%2Bshirts%26final_price_from%3D0%26final_price_to%3D1000000%26ref%3D%252Fparcourir%252Fchemises-pour-homme%252F%26page%3D'\n",
    "nb_products = 0\n",
    "for i in range (2, 100):\n",
    "    url = url_dico + str(i)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "    text_data = soup.text\n",
    "    dico = json.loads(text_data)\n",
    "    nb_products += len(dico['data']['feed_items'])\n",
    "print(\"Total products found:\", nb_products)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems promising ! Now what we need to understand before launching the definitive scraper is how the name of the html page containing the dictionary is chosen for each category. We first notice that this part 'https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2Fparcourir%2F' is common to all the categories. And the next part, starting by '_url=_' is just the url of the category of subcategory, with all \"/\" being replaced by \"%\". So we have done all the work already actually, this is free meal !\n",
    "\n",
    "We are actually now using Lyst's __API__ do get the data, which a good behaviour __usually__. If you can, always use provided API to scrap data. For instance, using Firefox, I can load the html page in a beautiful Json format, which allows me to see quickly how the data is organized. What is funny here is that LYst probably doesn't want to provide this API, it is linkely to be a private API for their engineers to organize their data. \n",
    "\n",
    "We can now get all the images using this API quite simply. Again, we test it on the category shirt. As all the infos (urls of thumbnail images, long description...) are not available in the API, we're going to get the url of the product and use our Beautiful Soup parser to retrieve the data we can't get through the API. So our \"naive\" work is useful after all ! We expects our scraper to be faster, as we skip the (long) infinite scroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scraper using the API on the category \"Shirts\"\n",
    "\n",
    "# Create a unique list of entries for all the products\n",
    "dico_final = []\n",
    "\n",
    "# Number of image total\n",
    "total_img = 0\n",
    "\n",
    "Base_api = 'https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2F'\n",
    "Base = 'https://www.lyst.fr'\n",
    "to_scrap = words_to_search['shirts']\n",
    "nb_products = 0\n",
    "# Loop on the subcategories\n",
    "for clothing in to_scrap:\n",
    "    url_true = \"parcourir/vetements-pour-homme/?category=\" + 'shirts' +  \"&subcategory=\" + clothing + \"&page=\"\n",
    "    url_api_base = Base_api + url_true.replace(\"+\", \"%2B\").replace(\"&\", \"%26\").replace(\"=\", \"%3D\").replace(\"/\", \"%2F\").replace(\"?\", \"%3F\")\n",
    "    cpt = 0\n",
    "    for i in range(2, 100):\n",
    "        print(\"Parsing page no\", i, \"/\", 100, \"...\")\n",
    "        url = url_api_base + str(i)\n",
    "        try:\n",
    "            html = requests.get(url)\n",
    "        except Execption as e:\n",
    "            print(\"Error: can't reach.\", e)\n",
    "            continue\n",
    "        soup = BeautifulSoup(html.text, 'lxml')\n",
    "        text_data = soup.text\n",
    "        dico = json.loads(text_data)\n",
    "        num = 0\n",
    "        for product in dico['data']['feed_items']:\n",
    "            time.sleep(0.1)\n",
    "            num +=1\n",
    "            dico_product = {}\n",
    "            dico_product['id'] = product['product_card']['id']\n",
    "            url_product = Base + product['product_card']['url']\n",
    "            html_product = requests.get(url_product)\n",
    "            soup_product = BeautifulSoup(html_product.text, 'lxml')\n",
    "            print(\"Extracting data for product no\", num)\n",
    "            \n",
    "            \n",
    "            # We retrieve all the data\n",
    "            if soup_product.find('div', {'itemprop':'brand'}) is not None:\n",
    "                if len(soup_product.find('div', {'itemprop':'brand'}).contents)>1:\n",
    "                    name_brand = soup_product.find('div', {'itemprop':'brand'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                    dico_product['brand'] = unidecode.unidecode(name_brand)\n",
    "            if soup_product.find('div', {'itemprop':'name'}) is not None:\n",
    "                short_description = soup_product.find('div', {'itemprop':'name'}).contents[0].string.replace(\"\\n\", \"\")\n",
    "                dico_product['short-description'] = unidecode.unidecode(short_description)\n",
    "            if soup_product.find('a', {'reason':'retailer-link'}) is not None:\n",
    "                retailer = soup_product.find('a', {'reason':'retailer-link'}).contents[1].string[19::].replace(\"\\n\", \"\")\n",
    "                dico_product['retailer'] = unidecode.unidecode(retailer)\n",
    "            if soup_product.find('div', {'class':'product-description__details text-paragraph mb0'}) is not None:\n",
    "                long_description = soup_product.find('div', {'class':'product-description__details text-paragraph mb0'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                dico_product['long-description'] = unidecode.unidecode(long_description)\n",
    "                \n",
    "            # Download the main image with the name of the retailer in its name\n",
    "            if soup_product.find('img', {'class':'image-gallery-main-img'}) is not None:\n",
    "                url_main_img = soup_product.find('img', {'class':'image-gallery-main-img'})['src'].replace(\"\\n\", \"\")\n",
    "                image_name = str(dico_product[\"id\"]) + url_main_img.split(\"/\")[-1] \n",
    "                file_name =DownloadPath + '/'+ 'shirts' + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_main_\" + image_name\n",
    "                try:\n",
    "                    r = requests.get(url_main_img, stream=True, headers=headers)\n",
    "                    f = open(file_name, 'wb')\n",
    "                    shutil.copyfileobj(r.raw, f)\n",
    "                    dico_product['url-main-image'] = url_main_img\n",
    "                    cpt +=1\n",
    "                except Exception as e:\n",
    "                    print(\"Error while downloading the image:\", e)\n",
    "                    continue\n",
    "            else:\n",
    "                dico_product = {}\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "        \n",
    "            # Download the thumbnail image also with the name of the retailer in their name\n",
    "            other_imgs = soup_product.find('div', {'is':'gallery-thumbnails'})\n",
    "            dico_product['other-images-url'] = []\n",
    "        \n",
    "            # Not all images have thumbnails images\n",
    "            if other_imgs is not None:\n",
    "                other_imgs = soup_product.find('div', {'is':'gallery-thumbnails'}).contents\n",
    "                nb_imgs = int(len(other_imgs)/2)\n",
    "                for i in range(1, nb_imgs):\n",
    "                    dico_product['other-images-url'].append(other_imgs[2*i+1]['href'])\n",
    "                    image_name = str(dico_product[\"id\"]) + other_imgs[2*i+1]['href'].split(\"/\")[-1]\n",
    "                    file_name =DownloadPath + '/' + 'shirts'+ '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_\" + str(i) + image_name\n",
    "                    if (other_imgs[2*i+1]['href'].replace(\"\\n\", \"\") != url_main_img):\n",
    "                        try:\n",
    "                            r = requests.get(other_imgs[2*i+1]['href'].replace(\"\\n\", \"\"), stream=True, headers=headers)\n",
    "                            f = open(file_name, 'wb')\n",
    "                            shutil.copyfileobj(r.raw, f)\n",
    "                            cpt +=1\n",
    "                        except Exception as e:\n",
    "                            print(\"Error while downloading the image:\", e)\n",
    "                            continue\n",
    "            # Append the cio of your products to your final list\n",
    "            dico_final.append(dico_product)\n",
    "            total_img += cpt\n",
    "            \n",
    "    # Count the number of images for this subcategory\n",
    "    print(\"Number of images downloaded for subcategory\", clothing, \":\", cpt)\n",
    "    \n",
    "# Total images downloaded\n",
    "print(\"Total images downloaded:\", total_img)\n",
    "    \n",
    "\n",
    "# Save all to  a json file\n",
    "with open(\"lyst_shirts.json\", \"w\") as fout:\n",
    "    json.dump(dico, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works ! We can now write the complete scraper for mensware using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final scraper for Lyst men's catalogue\n",
    "\n",
    "# We create 2 save lists: 1 for all the product, and 1 for each category in case an issue occurs.\n",
    "dico_final = []\n",
    "total_imgs = 0\n",
    "Base_api = 'https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2F'\n",
    "Base = 'https://www.lyst.fr'\n",
    "\n",
    "# Let's loop on all the words to search\n",
    "for category in words_to_search:\n",
    "    print(\"Processing category\", category, \"...\")\n",
    "    dico_category = []\n",
    "    total_cat = 0\n",
    "    subcategories = words_to_search[category]\n",
    "\n",
    "    # Loop on the subcategories\n",
    "    for clothing in subcategories:\n",
    "        print(\"Processing subcategory\", clothing, \"...\")\n",
    "        url_true = \"parcourir/vetements-pour-homme/?category=\" + category +  \"&subcategory=\" + clothing + \"&page=\"\n",
    "        url_api_base = Base_api + url_true.replace(\"+\", \"%2B\").replace(\"&\", \"%26\").replace(\"=\", \"%3D\").replace(\"/\", \"%2F\").replace(\"?\", \"%3F\")\n",
    "        cpt = 0\n",
    "        for i in range(2, 100):\n",
    "            print(\"Parsing page no\", i, \"/\", 100, \"...\")\n",
    "            url = url_api_base + str(i)\n",
    "            try:\n",
    "                html = requests.get(url)\n",
    "            except Execption as e:\n",
    "                print(\"Error: can't reach.\", e)\n",
    "                continue\n",
    "            soup = BeautifulSoup(html.text, 'lxml')\n",
    "            text_data = soup.text\n",
    "            dico = json.loads(text_data)\n",
    "            num = 0\n",
    "            for product in dico['data']['feed_items']:\n",
    "                time.sleep(0.1)\n",
    "                num +=1\n",
    "                dico_product = {}\n",
    "                dico_product['id'] = product['product_card']['id']\n",
    "                url_product = Base + product['product_card']['url']\n",
    "                html_product = requests.get(url_product)\n",
    "                soup_product = BeautifulSoup(html_product.text, 'lxml')\n",
    "                print(\"Extracting data for product no\", num)\n",
    "            \n",
    "            \n",
    "                # We retrieve all the data\n",
    "                if soup_product.find('div', {'itemprop':'brand'}) is not None:\n",
    "                    if len(soup_product.find('div', {'itemprop':'brand'}).contents)>1:\n",
    "                        name_brand = soup_product.find('div', {'itemprop':'brand'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                        dico_product['brand'] = unidecode.unidecode(name_brand)\n",
    "                if soup_product.find('div', {'itemprop':'name'}) is not None:\n",
    "                    short_description = soup_product.find('div', {'itemprop':'name'}).contents[0].string.replace(\"\\n\", \"\")\n",
    "                    dico_product['short-description'] = unidecode.unidecode(short_description)\n",
    "                if soup_product.find('a', {'reason':'retailer-link'}) is not None:\n",
    "                    retailer = soup_product.find('a', {'reason':'retailer-link'}).contents[1].string[19::].replace(\"\\n\", \"\")\n",
    "                    dico_product['retailer'] = unidecode.unidecode(retailer)\n",
    "                if soup_product.find('div', {'class':'product-description__details text-paragraph mb0'}) is not None:\n",
    "                    long_description = soup_product.find('div', {'class':'product-description__details text-paragraph mb0'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                    dico_product['long-description'] = unidecode.unidecode(long_description)\n",
    "                \n",
    "                # Download the main image with the name of the retailer in its name\n",
    "                if soup_product.find('img', {'class':'image-gallery-main-img'}) is not None:\n",
    "                    url_main_img = soup_product.find('img', {'class':'image-gallery-main-img'})['src'].replace(\"\\n\", \"\")\n",
    "                    image_name = str(dico_product[\"id\"]) + \"_\" + url_main_img.split(\"/\")[-1]\n",
    "                    file_name =DownloadPath + '/'+ category + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_main_\" + image_name\n",
    "                    try:\n",
    "                        r = requests.get(url_main_img, stream=True, headers=headers)\n",
    "                        f = open(file_name, 'wb')\n",
    "                        shutil.copyfileobj(r.raw, f)\n",
    "                        dico_product['url-main-image'] = url_main_img\n",
    "                        cpt +=1\n",
    "                    except Exception as e:\n",
    "                        print(\"Error while downloading the image:\", e)\n",
    "                        continue\n",
    "                else:\n",
    "                    dico_product = {}\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # Download the thumbnail image also with the name of the retailer in their name\n",
    "                other_imgs = soup_product.find('div', {'is':'gallery-thumbnails'})\n",
    "                dico_product['other-images-url'] = []\n",
    "        \n",
    "                # Not all images have thumbnails images\n",
    "                if other_imgs is not None:\n",
    "                    other_imgs = soup_product.find('div', {'is':'gallery-thumbnails'}).contents\n",
    "                    nb_imgs = int(len(other_imgs)/2)\n",
    "                    for i in range(1, nb_imgs):\n",
    "                        dico_product['other-images-url'].append(other_imgs[2*i+1]['href'])\n",
    "                        image_name = str(dico_product[\"id\"]) + \"_\" + other_imgs[2*i+1]['href'].split(\"/\")[-1]\n",
    "                        file_name =DownloadPath + '/' + category + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_\" + str(i) + image_name\n",
    "                        if (other_imgs[2*i+1]['href'].replace(\"\\n\", \"\") != url_main_img):\n",
    "                            try:\n",
    "                                r = requests.get(other_imgs[2*i+1]['href'].replace(\"\\n\", \"\"), stream=True, headers=headers)\n",
    "                                f = open(file_name, 'wb')\n",
    "                                shutil.copyfileobj(r.raw, f)\n",
    "                                cpt +=1\n",
    "                            except Exception as e:\n",
    "                                print(\"Error while downloading the image:\", e)\n",
    "                                continue\n",
    "                # Append the cio of your products to your final list\n",
    "                dico_final.append(dico_product)\n",
    "                dico_category.append(dico_product)\n",
    "        total_cat += cpt\n",
    "            \n",
    "        # Count the number of images for this subcategory\n",
    "        print(\"Number of images downloaded for subcategory\", clothing, \":\", cpt)\n",
    "    \n",
    "    # Total images downloaded for this category \n",
    "    print(\"Total images downloaded for category\", category, \":\", total_cat)\n",
    "    total_imgs += total_cat\n",
    "    \n",
    "\n",
    "    # Save all the image of the category to  a json file\n",
    "    name = \"lyst\"+ category + \".json\"\n",
    "    with open(name, \"w\") as fout:\n",
    "        json.dump(dico_category, fout)\n",
    "\n",
    "# Total images downloaded \n",
    "print(\"Total images downloaded:\", total_imgs)\n",
    "# Finally, create the final json\n",
    "with open(\"lyst_total.json\", \"w\") as fout:\n",
    "    json.dump(dico_final, fout)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the same with the women's catalogue, we just need to change a few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the location to save the data\n",
    "DownloadPath = \"/media/arthur/DATA/Databases/LystScrapingF\"\n",
    "\n",
    "# We create a dictionary with all cat. and subcat. of the Lyst catalogue\n",
    "words_to_search_f =  {\n",
    "    'hosiery':['stocking','socks', 'tights'],\n",
    "    'jumpsuits':['full+length+jumpsuits', 'playsuits'],\n",
    "    'skirts':['maxi+skirts', 'knee+length+skirts', 'mid+length+skirts', 'mini+skirts'],\n",
    "    'lingerie':['bodysuits', 'camisoles', 'panties','sets','corsetry','bras'],\n",
    "    'coats':['capes','trench+coats','short+coats','fur+coats','long+coats','parka+coats'],\n",
    "    'jeans':['bootcut+jeans', 'straight+jeans', 'wide-leg+jeans', 'flared+jeans', 'skinny+jeans', 'cropped+jeans'],\n",
    "    'coats':['trench+coats', 'short+coats', 'long+coats',  'parka+coats'],\n",
    "    'pants':['leggings','cropped+pants', 'straight-leg+pants', 'full+length+pants','skinny+pants', 'wide-leg+pants', 'harem+pants', 'cargo+pants'],\n",
    "    'beachwear':['bikinis', 'one-piece+swimsuits', 'sarongs','towels', 'kaftans'],\n",
    "    'knitwear':['cardigans','ponchos', 'sweaters', 'turtlenecks', 'zipped+sweaters', 'sleeveless+sweaters'],\n",
    "    'dresses':['mini+dresses', 'cocktail+dresses','gowns','casual+dresses','formal+dresses','maxi+dresses'],\n",
    "    'shorts':['bermudas+shorts','cargo+shorts','denim+shorts', 'mini+shorts','formal+shorts', 'knee+length+shorts'],\n",
    "    'sweats':['sweatpants','tracksuits','sweatshirts', 'hoodies'],\n",
    "    'tops':['shirts','blouses','short+sleeve+tops','long+sleeved+tops', 't-shirts','sleeveless+tops'],\n",
    "    'jackets':['formal+jackets','leather+jackets','fur+jackets','denim+jackets','waistcoats', 'casual+jackets','parka+jackets'],\n",
    "    'nightwear':['nightgowns','robes','pyjamas'],\n",
    "    'suits':['skirt+suits', 'pant+suits']\n",
    "}\n",
    "\n",
    "# Create the tree in your hard disk \n",
    "for category in words_to_search_f:\n",
    "    os.makedirs(DownloadPath + \"/\" + category)\n",
    "    subcategories = words_to_search_f[category]\n",
    "    for clothing in subcategories:\n",
    "        os.makedirs(DownloadPath + \"/\" + category + \"/\" + clothing)\n",
    "print(\"Tree created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final scraper for womenswear on Lyst.fr\n",
    "\n",
    "# We create 2 save lists: 1 for all the product, and 1 for each category in case an issue occurs.\n",
    "dico_final_f = []\n",
    "total_imgs_f = 0\n",
    "Base_api = 'https://www.lyst.fr/api/rothko/modules/product_feed/?url=%2F'\n",
    "Base = 'https://www.lyst.fr'\n",
    "\n",
    "# Let's loop on all the words to search\n",
    "for category in words_to_search_f:\n",
    "    print(\"Processing category\", category, \"...\")\n",
    "    dico_category = []\n",
    "    total_cat = 0\n",
    "    subcategories = words_to_search_f[category]\n",
    "\n",
    "    # Loop on the subcategories\n",
    "    for clothing in subcategories:\n",
    "        print(\"Processing subcategory\", clothing, \"...\")\n",
    "        url_true = \"parcourir/vetements/?category=\" + category +  \"&subcategory=\" + clothing + \"&page=\"\n",
    "        print(url_true)\n",
    "        url_api_base = Base_api + url_true.replace(\"+\", \"%2B\").replace(\"&\", \"%26\").replace(\"=\", \"%3D\").replace(\"/\", \"%2F\").replace(\"?\", \"%3F\")\n",
    "        cpt = 0\n",
    "        for i in range(2, 100):\n",
    "            print(\"Parsing page no\", i, \"/\", 100, \"...\")\n",
    "            url = url_api_base + str(i)\n",
    "            try:\n",
    "                html = requests.get(url)\n",
    "            except Execption as e:\n",
    "                print(\"Error: can't reach.\", e)\n",
    "                continue\n",
    "            soup = BeautifulSoup(html.text, 'lxml')\n",
    "            text_data = soup.text\n",
    "            dico = json.loads(text_data)\n",
    "            num = 0\n",
    "            for product in dico['data']['feed_items']:\n",
    "                time.sleep(0.1)\n",
    "                num +=1\n",
    "                dico_product = {}\n",
    "                dico_product['id'] = product['product_card']['id']\n",
    "                url_product = Base + product['product_card']['url']\n",
    "                html_product = requests.get(url_product)\n",
    "                soup_product = BeautifulSoup(html_product.text, 'lxml')\n",
    "                print(\"Extracting data for product no\", num)\n",
    "            \n",
    "            \n",
    "                # We retrieve all the data\n",
    "                if soup_product.find('div', {'itemprop':'brand'}) is not None:\n",
    "                    if len(soup_product.find('div', {'itemprop':'brand'}).contents)>1:\n",
    "                        name_brand = soup_product.find('div', {'itemprop':'brand'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                        dico_product['brand'] = unidecode.unidecode(name_brand)\n",
    "                if soup_product.find('div', {'itemprop':'name'}) is not None:\n",
    "                    short_description = soup_product.find('div', {'itemprop':'name'}).contents[0].string.replace(\"\\n\", \"\")\n",
    "                    dico_product['short-description'] = unidecode.unidecode(short_description)\n",
    "                if soup_product.find('a', {'reason':'retailer-link'}) is not None:\n",
    "                    retailer = soup_product.find('a', {'reason':'retailer-link'}).contents[1].string[19::].replace(\"\\n\", \"\")\n",
    "                    dico_product['retailer'] = unidecode.unidecode(retailer)\n",
    "                if soup_product.find('div', {'class':'product-description__details text-paragraph mb0'}) is not None:\n",
    "                    long_description = soup_product.find('div', {'class':'product-description__details text-paragraph mb0'}).contents[1].string.replace(\"\\n\", \"\")\n",
    "                    dico_product['long-description'] = unidecode.unidecode(long_description)\n",
    "                \n",
    "                # Download the main image with the name of the retailer in its name\n",
    "                if soup_product.find('img', {'class':'image-gallery-main-img'}) is not None:\n",
    "                    url_main_img = soup_product.find('img', {'class':'image-gallery-main-img'})['src'].replace(\"\\n\", \"\")\n",
    "                    image_name = str(dico_product[\"id\"]) + \"_\" + url_main_img.split(\"/\")[-1] \n",
    "                    file_name =DownloadPath + '/'+ category + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_main_\" + image_name\n",
    "                    try:\n",
    "                        r = requests.get(url_main_img, stream=True, headers=headers)\n",
    "                        f = open(file_name, 'wb')\n",
    "                        shutil.copyfileobj(r.raw, f)\n",
    "                        dico_product['url-main-image'] = url_main_img\n",
    "                        cpt +=1\n",
    "                    except Exception as e:\n",
    "                        print(\"Error while downloading the image:\", e)\n",
    "                        continue\n",
    "                else:\n",
    "                    dico_product = {}\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # Download the thumbnail image also with the name of the retailer in their name\n",
    "                other_imgs = soup_product.find('div', {'is':'gallery-thumbnails'})\n",
    "                dico_product['other-images-url'] = []\n",
    "        \n",
    "                # Not all images have thumbnails images\n",
    "                if other_imgs is not None:\n",
    "                    other_imgs = soup_product.find('div', {'is':'gallery-thumbnails'}).contents\n",
    "                    nb_imgs = int(len(other_imgs)/2)\n",
    "                    for i in range(1, nb_imgs):\n",
    "                        dico_product['other-images-url'].append(other_imgs[2*i+1]['href'])\n",
    "                        image_name = str(dico_product[\"id\"]) + \"_\" + other_imgs[2*i+1]['href'].split(\"/\")[-1]\n",
    "                        file_name =DownloadPath + '/' + category + '/' + clothing + \"/\" + retailer.replace(\" \", \"_\") + \"_\" + str(i) + image_name\n",
    "                        if (other_imgs[2*i+1]['href'].replace(\"\\n\", \"\") != url_main_img):\n",
    "                            try:\n",
    "                                r = requests.get(other_imgs[2*i+1]['href'].replace(\"\\n\", \"\"), stream=True, headers=headers)\n",
    "                                f = open(file_name, 'wb')\n",
    "                                shutil.copyfileobj(r.raw, f)\n",
    "                                cpt +=1\n",
    "                            except Exception as e:\n",
    "                                print(\"Error while downloading the image:\", e)\n",
    "                                continue\n",
    "                # Append the cio of your products to your final list\n",
    "                dico_final_f.append(dico_product)\n",
    "                dico_category.append(dico_product)\n",
    "        total_cat += cpt\n",
    "            \n",
    "        # Count the number of images for this subcategory\n",
    "        print(\"Number of images downloaded for subcategory\", clothing, \":\", cpt)\n",
    "    \n",
    "    \n",
    "    # Total images downloaded for this category \n",
    "    print(\"Total images downloaded for category\", category, \":\", total_cat)\n",
    "    total_imgs_f += total_cat\n",
    "    \n",
    "\n",
    "    # Save all the image of the category to  a json file\n",
    "    name = \"lyst_f\"+ category + \".json\"\n",
    "    with open(name, \"w\") as fout:\n",
    "        json.dump(dico_category, fout)\n",
    "        \n",
    "# Total image downloaded\n",
    "print(\"Total image downloaded:\", total_imgs_f)\n",
    "\n",
    "# Finally, create the final json\n",
    "with open(\"lyst_total_f.json\", \"w\") as fout:\n",
    "    json.dump(dico_final_f, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it, you are ready to extract all the data we wanted. Now, there is still an issue, as downloading the image might take too much time depending on your Internet connexion and your hardware. As mine is not great and I don't use an SSD disk for this job, I will be using a __Virtual Machine (VM)__ to do this job. IN fact, from a performance point of view, the best solution would be to use several VM in parallel with different IPs to download all the data, this way you save a lot of time and shouldn't be banned. But as I said, remember to be polite when scraping data so I'll only use one Google Cloud instance. With the right parameters, these VMs are relatively cheap for such works.\n",
    "\n",
    "I hope you found this quick tutorial useful, as I said I am still a beginner so any feedback is very welcome ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
