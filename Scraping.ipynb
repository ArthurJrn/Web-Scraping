{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Sarenza (shoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scraper uses BeautifulSoup. Import the necessary packages and librairies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headers is the fingerprint of the web browser you use to perform scraping. In case you get bloced by anti-scraping, one solution is to rotate proxies and headers to disguise yourself as several regular user. Use sleep as well as false request in case the anti-scraper is smart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your main URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sarenza.com/tout-chaussure-homme\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the website for the html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a beautiful soup object. You can choose 'lxml' or 'hlml.parser' as a parser for your file depending on the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have an idea of how many images we can get from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5960 modèles\n"
     ]
    }
   ],
   "source": [
    "number = soup.findAll('div', {\"class\":\"count\"})[0].text\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the type of product we're gonna get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaussure homme\n"
     ]
    }
   ],
   "source": [
    "type = soup.find('h1',{\"class\":\"title-level1\"}).text\n",
    "print(type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna parse through the page showing all the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start scraping. We gonna clik on every product and gather some useful infos such as the description, the brand, download image of the product... We gather the url, download the image and put the infos in a .json file for a later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping image no 1\n",
      "Scraping image no 2\n",
      "Scraping image no 3\n",
      "Scraping image no 4\n",
      "Scraping image no 5\n",
      "Scraping image no 6\n",
      "Scraping image no 7\n",
      "Scraping image no 8\n",
      "Scraping image no 9\n",
      "Scraping image no 10\n",
      "Scraping image no 11\n",
      "Scraping image no 12\n",
      "Scraping image no 13\n",
      "Scraping image no 14\n",
      "Scraping image no 15\n",
      "Scraping image no 16\n",
      "Scraping image no 17\n",
      "Scraping image no 18\n",
      "Scraping image no 19\n",
      "Scraping image no 20\n",
      "Scraping image no 21\n",
      "Scraping image no 22\n",
      "Scraping image no 23\n",
      "Scraping image no 24\n",
      "Scraping image no 25\n",
      "Scraping image no 26\n",
      "Scraping image no 27\n",
      "Scraping image no 28\n",
      "Scraping image no 29\n",
      "Scraping image no 30\n",
      "Scraping image no 31\n",
      "Scraping image no 32\n",
      "Scraping image no 33\n",
      "Scraping image no 34\n",
      "Scraping image no 35\n",
      "Scraping image no 36\n",
      "Scraping image no 37\n",
      "Scraping image no 38\n",
      "Scraping image no 39\n",
      "Scraping image no 40\n",
      "Scraping image no 41\n",
      "Scraping image no 42\n",
      "Scraping image no 43\n",
      "Scraping image no 44\n",
      "Scraping image no 45\n",
      "Scraping image no 46\n",
      "Scraping image no 47\n",
      "Scraping image no 48\n",
      "Scraping image no 49\n",
      "Scraping image no 50\n",
      "Scraping image no 51\n",
      "Scraping image no 52\n",
      "Scraping image no 53\n",
      "Scraping image no 54\n",
      "Scraping image no 55\n",
      "Scraping image no 56\n",
      "Scraping image no 57\n",
      "Scraping image no 58\n",
      "Scraping image no 59\n",
      "Scraping image no 60\n",
      "Scraping image no 61\n",
      "Scraping image no 62\n",
      "Scraping image no 63\n",
      "Scraping image no 64\n",
      "Scraping image no 65\n",
      "Scraping image no 66\n",
      "Scraping image no 67\n",
      "Scraping image no 68\n",
      "Scraping image no 69\n",
      "Scraping image no 70\n",
      "Scraping image no 71\n",
      "Scraping image no 72\n",
      "Scraping image no 73\n",
      "Scraping image no 74\n",
      "Scraping image no 75\n",
      "Scraping image no 76\n",
      "Scraping image no 77\n",
      "Scraping image no 78\n",
      "Scraping image no 79\n",
      "Scraping image no 80\n",
      "Scraping image no 81\n",
      "Scraping image no 82\n",
      "Scraping image no 83\n",
      "Scraping image no 84\n",
      "Scraping image no 85\n",
      "Scraping image no 86\n",
      "Scraping image no 87\n",
      "Scraping image no 88\n",
      "Scraping image no 89\n",
      "Scraping image no 90\n",
      "Scraping image no 91\n",
      "Scraping image no 92\n",
      "Scraping image no 93\n",
      "Scraping image no 94\n",
      "Scraping image no 95\n",
      "Scraping image no 96\n",
      "Scraping image no 97\n",
      "Scraping image no 98\n",
      "Scraping image no 99\n",
      "out\n"
     ]
    }
   ],
   "source": [
    "links = soup.findAll('a', {'class':'product-link'})\n",
    "i = 0\n",
    "urls = []\n",
    "infos = []\n",
    "for product in links:\n",
    "    i += 1\n",
    "    link_to_product = product['href']\n",
    "    Name = (\"http://www.sarenza.com\" + link_to_product)\n",
    "    html_page = requests.get(Name)\n",
    "    soupProduct = BeautifulSoup(html_page.text, 'lxml')\n",
    "    print(\"Scraping image no\", i)\n",
    "    \n",
    "    #Get the url of the images of the product\n",
    "    img = soupProduct.findAll(\"img\", {\"class\":\"zoom progressive\"}, {\"width\": \"480\"})\n",
    "    \n",
    "    #Tip: change the end of the url to dowload a larger image\n",
    "    url_img = img[0]['src']\n",
    "    url_img = url_img[:-7]\n",
    "    url_img = url_img + \"5\"\n",
    "    url_img = url_img + \"0\"\n",
    "    url_img = url_img + \"0\"\n",
    "    url_img = url_img + \":\"\n",
    "    url_img = url_img + \"3\"\n",
    "    url_img = url_img + \"3\"\n",
    "    url_img = url_img + \"3\"\n",
    "    \n",
    "    urls.append(url_img)\n",
    "    \n",
    "    #Get all the infos about the product\n",
    "    types = soupProduct.findAll(\"div\", {\"class\":\"product-details\"})\n",
    "    recap = types[0].table.text\n",
    "    recap = recap.split(\"\\n\")\n",
    "    dico_prod = {\n",
    "        \"Ref\": 0,\n",
    "        \"Type\":\"\",\n",
    "        \"Saison\":\"\",\n",
    "        \"Largeur\":\"\",\n",
    "        \"Pays de fabrication\":\"\",\n",
    "        \"Pointure de ref\":0,\n",
    "        \"Construction\": \"\",\n",
    "        \"Couleur\": \"\",\n",
    "        \"Doublure\":\"\",\n",
    "        \"Hauteur du talon\": \"\",\n",
    "        \"Hauteur de tige\":\"\", \n",
    "        \"Largeur\":\"\",\n",
    "        \"Dessus\":\"\", \n",
    "        \"Semelle intérieure\": \"\",\n",
    "        \"Semelle extérieure\": \"\",\n",
    "        \"url\": \"\",\n",
    "    }\n",
    "    # Process the data a little bit\n",
    "    for elt in recap:\n",
    "        if re.search(\"Ref\", elt):\n",
    "            dico_prod[\"Ref\"] = float(elt.split(\" \")[1])\n",
    "        if re.search(\"Type\", elt):\n",
    "            dico_prod[\"Type\"] = elt[5:]\n",
    "        if re.search(\"Saison\", elt):\n",
    "            dico_prod[\"Saison\"] = elt[7:]\n",
    "        if re.search(\"Largeur\", elt):\n",
    "            dico_prod[\"Largeur\"] = elt.split(\" \")[1]\n",
    "        if re.search(\"Pays\", elt):\n",
    "            dico_prod[\"Pays de fabrication\"] = elt[20:]\n",
    "        if re.search(\"Pointure\", elt):\n",
    "            dico_prod[\"Pointure de ref\"] = float(elt.split(\" \")[3])\n",
    "        if re.search(\"Construction\", elt):\n",
    "            dico_prod[\"Construction\"]= elt[13:]\n",
    "        if re.search(\"Couleur\", elt):\n",
    "            dico_prod[\"Couleur\"] = elt.split(\" \")[1]\n",
    "        if re.search(\"Doublure\", elt):\n",
    "            dico_prod[\"Doublure\"] = elt.split(\" \")[1]\n",
    "        if re.search(\"talon\", elt):\n",
    "            dico_prod[\"Hauteur du talon\"] = elt[16:]\n",
    "        if re.search(\"tige\", elt):\n",
    "            dico_prod[\"Hauteur de tige\"] = elt[15:]\n",
    "        if re.search(\"Largeur\", elt):\n",
    "            dico_prod[\"Largeur\"] = elt.split(\" \")[1]\n",
    "        if re.search(\"Dessus\", elt):\n",
    "            dico_prod[\"Dessus\"] =  elt.split(\" \")[1]\n",
    "        if re.search(\"intérieure\", elt):\n",
    "            dico_prod[\"Semelle intérieure\"] = elt.split(\" \")[2]\n",
    "        if re.search(\"extérieure\", elt):\n",
    "            dico_prod[\"Semelle extérieure\"] = elt.split(\" \")[3]\n",
    "    dico_prod[\"url\"] = url_img\n",
    "    infos.append(dico_prod)\n",
    "    if i == 99:\n",
    "        break\n",
    "print(\"out\")\n",
    "\n",
    "# Save all infos in a json file\n",
    "\n",
    "with open(\"sarenza.json\", \"w\") as fout:\n",
    "    json.dump(infos, fout)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
